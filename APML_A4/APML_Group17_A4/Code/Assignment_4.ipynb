{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 â€“ Robot in a maze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the  fourth assignment, you will delve into the application of RL algorithms to address the real-world challenge of navigating a robot through a maze dubbed 'robot in a maze'. The primary objectives of this assignment include:\n",
    "\n",
    " -   Formalizing a practical problem into a Markov Decision Process (MDP).\n",
    " -   Gaining familiarity with the OpenAI Gym framework (recently renamed as Gymnasium) and utilizing it to implement RL agents.\n",
    " -   Applying SARSA and Q-learning algorithms to solve the 'robot in a maze' MDP problem.\n",
    " -   Evaluating the outcomes of the reinforcement learning process and interpreting your findings.\n",
    " -   Reflecting on the distinctions between the two types of RL algorithms employed.\n",
    "\n",
    "By accomplishing these objectives, you will not only enhance your understanding of RL algorithms but also develop practical skills in formulating and solving complex problems in the context of autonomous navigation within a maze.\n",
    "\n",
    "In this assignment, you will be developing a robot to navigate its way through a maze. The project is divided into three parts (5 subtasks).\n",
    "\n",
    "  1. In the first part, you will familiarize yourself with the OpenAI Gym/Gymnasium framework.\n",
    "  2. In the second part, we have implemented the environment for you based on the Gym/Gymnasium framework. Your tasks include:  \n",
    "     2.1. formalizing the problem as an MDP model,    \n",
    "     2.2. implementing your own RL agents, and      \n",
    "     2.3. training them to find the shortest route out of a maze.\n",
    "  3. Finally, in the third part, you will evaluate and interpret the results obtained from the implemented RL agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to explain how the RL framework of gym works. \n",
    "- An **ENVIRONMENT**, \n",
    "- You also have an **AGENT**,\n",
    "- In MDP problems, such as ours, the **ENVIRONMENT** provides an **OBSERVATION**, which represents the state of the **ENVIRONMENT** at the current moment.\n",
    "- The agent takes an **ACTION** based on its **OBSERVATION**,\n",
    "<!-- When a single **ACTION** is chosen and fed to our **ENVIRONMENT**, the **ENVIRONMENT** measures how good the action was taken and produces a **REWARD**, which is usually a numeric value. -->\n",
    "- When the agent takes an ACTION, the ENVIRONMENT assesses the effectiveness of the action and generates a REWARD, which is usually a numeric value.\n",
    "\n",
    "Please read the 'Basic usage' https://gymnasium.farama.org/content/basic_usage/ for better understanding the framework.  And do not forget import gymnasium before running other codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Tasks\n",
    "\n",
    "Next, you will tackle a practical Markov Decision Process (MDP) problem, the 'robot in a maze,' based on the gym framework. Your task involves implementing an RL agent and training it to discover the shortest route to achieve the maze goal. In this MDP, the environment is represented as a grid world (a maze), with the agent being a robot. At each time step, the robot begins at a random location and can move within the grid world. The overarching objective is to find the way out, reaching the final location. Consequently, you will need to identify a fixed goal position within the maze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Model the practical task into a MDP\n",
    "\n",
    "To solve a RL problem, we start with formalizing the problem into a MDP model. Please describe this MDP model in your report. \n",
    "\n",
    "Notice: No empricial data provided in this assignment, so the point of 'data description and exploration' will be given to this step. \n",
    "\n",
    "While exploring your MDP model, you shall think about questions such as:\n",
    "- What is the environment? How does it look like?\n",
    "- What simulated data can your RL agent observe from the environment? How does it look like?\n",
    "- Which data is considered as the state? Which data is considered as the reward?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the environment\n",
    "\n",
    "There is no need to implement your own environment; you should use the environment provided in the file **environment.py**. However, please ensure to take a look at it so that you understand the inner workings of this environment.\n",
    "\n",
    "The core gym interface is **Env**, which serves as the unified environment interface. The following are the Env methods you should be familiar with:\n",
    "\n",
    "- reset(self): Reset the environment's state and return the observation.\n",
    "- step(self, action): Advance the environment by one timestep and return the observation, reward, done, and info.\n",
    "- render(self, mode='rgb_array'): Render one frame of the environment. The default mode will produce something human-friendly, such as a pop up window. However, in this assignment, there is no need to create a pop-up window.\n",
    "\n",
    "Please note that you need to install the [mazelab](https://github.com/yupei-du/mazelab.git) package, from **Yupei Du's** repository, to run the environment (a file with required packages is also provided). If you run the cell below for the first time, make sure to restart the IPython notebook at least once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/yupei-du/mazelab.git\n",
    "#!pip install -e mazelab\n",
    "#!pip install pandas\n",
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now check whether the required packages (e.g. mazela, pandas, tqdm, seaborn) are installed. Please install the ones are missing. \n",
    "\n",
    "ATTENTION: To run the given code, please use the python version 3.7-3.9, and the numpy version < 1.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check Python version\n",
    "# import sys\n",
    "# print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# # Check NumPy version\n",
    "# import numpy as np\n",
    "# print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "#conda list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide a few helper functions to make it easier to debug your agents. \n",
    " - `animate_run` will enable you to see the agent's behavior. It takes a list of images which can be produced by the `env.render` function of the environment\n",
    " - `visualize_agent_brain` will provide you with a way to visualize the agents learned q_table. Use it after you have implemented and trained your agents. The first plot will show the highest q-value per state (position on the map) and the second will tell you which action the agent would choose at that state/position. It takes the environment and the agent as input.\n",
    "\n",
    "Below you will find a basic example of how the animation function works. Please notice that: whenever you **reset()** the environment, the agent will start at a random position (a different state). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The helper functions\n",
    "\n",
    "from IPython import get_ipython\n",
    "import random\n",
    "from mazelab.generators import random_maze, morris_water_maze\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from mazelab.solvers import dijkstra_solver\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from environment import TaskEnv\n",
    "from typing import Tuple, List\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def animate_run(data:List[np.ndarray]):\n",
    "    init_img = data[0]\n",
    "    remaining_img = data[1:]\n",
    "    img_container = plt.imshow(init_img)  # only call this once\n",
    "    for img in remaining_img:\n",
    "        img_container.set_data(img)  # just update the data\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "\n",
    "def visualize_agent_brain(agent, env: TaskEnv):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    ax1.set_title(\"Highest state value at position (x,y)\")\n",
    "    state_value_map = agent.q_table.max(axis=2)\n",
    "    sns.heatmap(state_value_map, ax=ax1)\n",
    "\n",
    "    ax2.set_title(\"Chosen action at position (x,y)\")\n",
    "    n = env.action_space.n + 1\n",
    "    path = env.maze.objects.free.positions\n",
    "    decisions_map = np.array([[x_, y_, agent.select_action([x_, y_]) + 1] for x_, y_ in path])\n",
    "    state_action_map = np.zeros_like(agent.q_table.max(axis=2))\n",
    "    state_action_map[decisions_map[:, 0], decisions_map[:, 1]] = decisions_map[:, 2]\n",
    "    cmap = sns.color_palette(\"viridis\", n)\n",
    "    sns.heatmap(state_action_map, cmap=cmap, ax=ax2)\n",
    "    colorbar = ax2.collections[0].colorbar\n",
    "    r = (colorbar.vmax) - colorbar.vmin\n",
    "    colorbar.set_ticks([colorbar.vmin + r / n * (0.5 + i) for i in range(n)])\n",
    "    colorbar.set_ticklabels(['N/A', 'north', 'south', 'west', 'east'])\n",
    "    fig.tight_layout()\n",
    "    return plt.show()\n",
    "\n",
    "\n",
    "env = TaskEnv()\n",
    "env.reset()\n",
    "impassable_array = env.unwrapped.maze.to_impassable()\n",
    "motions = env.unwrapped.motions\n",
    "start = env.unwrapped.maze.objects.agent.positions[0]\n",
    "goal = env.unwrapped.maze.objects.goal.positions[0]\n",
    "actions = dijkstra_solver(impassable_array, motions, start, goal)\n",
    "print(actions)\n",
    "\n",
    "imgs = []\n",
    "rewards = 0.0\n",
    "for action in actions:\n",
    "    _, reward, _, _ = env.step(action)\n",
    "    rewards += reward\n",
    "    imgs.append(env.render(\"rgb_array\"))\n",
    "print(rewards)\n",
    "\n",
    "animate_run(imgs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Implement the agents \n",
    "\n",
    "In this part, you are expected to implement two RL agents. \n",
    "\n",
    "- Agent 1 uses the Q-learning algorithm to learn the optimal solution\n",
    "- Agent 2 uses the SARSA algorithm to learn the optimal solution. To decide the action to take at each time step,  this agent uses the epsilon greedy action selection.\n",
    "\n",
    "Here, we have also provided an example agent: the Random Agent.  It follows a random policy to move at each step (randomly selecting an action). You can use this example agent as a baseline to evaluate your agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random agent\n",
    "class RandomAgent():\n",
    "    def __init__(self,\n",
    "                 env: TaskEnv,\n",
    "                 exploration_rate: float = None,\n",
    "                 learning_rate: float = None,\n",
    "                 discount_factor: float = None) -> int:\n",
    "        self.env = env\n",
    "        self.epsilon = 1  # A random agent \"explores\" always, so epsilon will be 1\n",
    "        self.alpha = 0  # A random agent never learns, so there's no need for a learning rate\n",
    "        self.gamma = 0  # A random agent does not update it's q-table. Hence, it's zero.\n",
    "        self.q_table = np.zeros(env.observation_space.shape + (env.action_space.n, ), dtype=float)\n",
    "        self.actions = env.action_space\n",
    "\n",
    "    def select_action(self, state: Tuple[int, int], use_greedy_strategy: bool = False) -> int:\n",
    "        if not use_greedy_strategy:\n",
    "            if random.random() < self.epsilon:\n",
    "                next_action = self.actions.sample()\n",
    "                return next_action\n",
    "\n",
    "        x, y = state\n",
    "        max_val = np.max(self.q_table[x, y, :])\n",
    "        find_max_val = np.where(self.q_table[x, y, :] == max_val)\n",
    "        next_action = np.random.choice(find_max_val[0])\n",
    "        return next_action\n",
    "\n",
    "    def learn(self, state, action, next_state, reward, done):\n",
    "        return None\n",
    "    \n",
    "    def train(self, n_episodes=10_000):\n",
    "        rewards_per_episode = []\n",
    "        for episode in tqdm(range(n_episodes)):\n",
    "            obs = self.env.reset()\n",
    "            done = False\n",
    "            total_rewards = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.select_action(obs)\n",
    "                next_obs, reward, done, info = self.env.step(action)\n",
    "                total_rewards += reward\n",
    "                self.learn(obs, action, next_obs, reward, done)\n",
    "\n",
    "                obs = next_obs\n",
    "\n",
    "            rewards_per_episode.append(total_rewards)\n",
    "        \n",
    "        return rewards_per_episode\n",
    "\n",
    "    def reset(self):\n",
    "        self.q_table = np.zeros(env.observation_space.shape + (env.action_space.n, ), dtype=float)\n",
    "\n",
    "\n",
    "# TODO: implement two agents   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning agent\n",
    "class QLearningAgent():\n",
    "    def __init__(self,\n",
    "                 env: TaskEnv,\n",
    "                 exploration_rate: float = None,\n",
    "                 learning_rate: float = None,\n",
    "                 discount_factor: float = None) -> int:\n",
    "        self.env = env\n",
    "        self.epsilon = exploration_rate  # A random agent \"explores\" always, so epsilon will be 1\n",
    "        self.alpha = learning_rate  # A random agent never learns, so there's no need for a learning rate\n",
    "        self.gamma = discount_factor  # A random agent does not update it's q-table. Hence, it's zero.\n",
    "        self.q_table = np.zeros(env.observation_space.shape + (env.action_space.n, ), dtype=float)\n",
    "        self.actions = env.action_space\n",
    "\n",
    "    def select_action(self, state: Tuple[int, int], use_greedy_strategy: bool = False) -> int:\n",
    "        if not use_greedy_strategy:\n",
    "            if random.random() < self.epsilon:\n",
    "                next_action = self.actions.sample()\n",
    "                return next_action\n",
    "\n",
    "        x, y = state\n",
    "        max_val = np.max(self.q_table[x, y, :])\n",
    "        find_max_val = np.where(self.q_table[x, y, :] == max_val)\n",
    "        next_action = np.random.choice(find_max_val[0])\n",
    "        return next_action\n",
    "\n",
    "    def learn(self, state, action, next_state, reward, done):\n",
    "        x, y = state\n",
    "        next_x, next_y = next_state\n",
    "        old_value = self.q_table[x, y, action]\n",
    "        next_max = np.max(self.q_table[next_x, next_y, :])\n",
    "        \n",
    "        self.q_table[x, y, action] = old_value + self.alpha * (reward + self.gamma * next_max - old_value)\n",
    "    \n",
    "    def train(self, n_episodes=10_000):\n",
    "        rewards_per_episode = []\n",
    "        for episode in tqdm(range(n_episodes)):\n",
    "            obs = self.env.reset()\n",
    "            done = False\n",
    "            total_rewards = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.select_action(obs)\n",
    "                next_obs, reward, done, info = self.env.step(action)\n",
    "                total_rewards += reward\n",
    "                self.learn(obs, action, next_obs, reward, done)\n",
    "\n",
    "                obs = next_obs\n",
    "\n",
    "            rewards_per_episode.append(total_rewards)\n",
    "        \n",
    "        return rewards_per_episode\n",
    "\n",
    "    def reset(self):\n",
    "        self.q_table = np.zeros(env.observation_space.shape + (env.action_space.n, ), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarsa agent\n",
    "class SarsaAgent():\n",
    "    def __init__(self,\n",
    "                 env: TaskEnv,\n",
    "                 exploration_rate: float = None,\n",
    "                 learning_rate: float = None,\n",
    "                 discount_factor: float = None) -> int:\n",
    "        self.env = env\n",
    "        self.epsilon = exploration_rate  # A random agent \"explores\" always, so epsilon will be 1\n",
    "        self.alpha = learning_rate  # A random agent never learns, so there's no need for a learning rate\n",
    "        self.gamma = discount_factor  # A random agent does not update it's q-table. Hence, it's zero.\n",
    "        self.q_table = np.zeros(env.observation_space.shape + (env.action_space.n, ), dtype=float)\n",
    "        self.actions = env.action_space\n",
    "\n",
    "    def select_action(self, state: Tuple[int, int], use_greedy_strategy: bool = False) -> int:\n",
    "        if not use_greedy_strategy:\n",
    "            if random.random() < self.epsilon:\n",
    "                next_action = self.actions.sample()\n",
    "                return next_action\n",
    "\n",
    "        x, y = state\n",
    "        max_val = np.max(self.q_table[x, y, :])\n",
    "        find_max_val = np.where(self.q_table[x, y, :] == max_val)\n",
    "        next_action = np.random.choice(find_max_val[0])\n",
    "        return next_action\n",
    " \n",
    "\n",
    "    def learn(self, state, action, next_state, reward, next_action, done):\n",
    "        x, y = state\n",
    "        next_x, next_y = next_state\n",
    "        \n",
    "        old_value = self.q_table[x, y, action]\n",
    "        next_value = self.q_table[next_x, next_y, next_action] if not done else 0\n",
    "        \n",
    "        self.q_table[x, y, action] = old_value + self.alpha * (reward + self.gamma * next_value - old_value)\n",
    "    \n",
    "    def train(self, n_episodes=10_000):\n",
    "        rewards_per_episode = []\n",
    "        for episode in tqdm(range(n_episodes)):\n",
    "            obs = self.env.reset()\n",
    "            done = False\n",
    "            total_rewards = 0\n",
    "\n",
    "            action = self.select_action(obs)\n",
    "            while not done:\n",
    "                next_obs, reward, done, info = self.env.step(action)\n",
    "                total_rewards += reward\n",
    "                next_action = self.select_action(next_obs) \n",
    "                \n",
    "                self.learn(obs, action, next_obs, reward, next_action, done)\n",
    "\n",
    "                obs = next_obs\n",
    "                action = next_action\n",
    "\n",
    "            rewards_per_episode.append(total_rewards)\n",
    "        return rewards_per_episode\n",
    "                \n",
    "    def reset(self):\n",
    "        self.q_table = np.zeros(env.observation_space.shape + (env.action_space.n, ), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# TRAINING Agents #################\n",
    "# Handige link: https://gymnasium.farama.org/introduction/train_agent/\n",
    "\n",
    "def_exploration = 0.2\n",
    "def_learning = 0.01\n",
    "def_discount = 0.95\n",
    "train_env = TaskEnv()\n",
    "q_agent = QLearningAgent(train_env, \n",
    "                       exploration_rate = def_exploration, \n",
    "                       learning_rate    = def_learning, \n",
    "                       discount_factor  = def_discount\n",
    "                      )\n",
    "\n",
    "sarsa_agent = SarsaAgent(train_env, \n",
    "                       exploration_rate = def_exploration, \n",
    "                       learning_rate    = def_learning, \n",
    "                       discount_factor  = def_discount\n",
    "                      )\n",
    "\n",
    "q_agent.train()\n",
    "sarsa_agent.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Run the simulation\n",
    "\n",
    "Now, you write code for running a simulation. In each run, you shall setup the epsilon parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run the simulation\n",
    "def runSim(agent):\n",
    "    obs = agent.env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    imgs = []\n",
    "\n",
    "    while not done:\n",
    "        # Remove exploration during testing\n",
    "        action = agent.select_action(obs, use_greedy_strategy=True)\n",
    "        \n",
    "        next_obs, reward, done, info = agent.env.step(action)\n",
    "        print(reward)\n",
    "        total_reward += reward\n",
    "\n",
    "        imgs.append(agent.env.render(\"rgb_array\"))\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "    animate_run(imgs)\n",
    "    plt.show()\n",
    "    \n",
    "    visualize_agent_brain(agent, agent.env)\n",
    "\n",
    "    print(\"Total reward:\", total_reward)\n",
    "\n",
    "    return total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runSim(q_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runSim(sarsa_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Play with parameters and analyse results\n",
    " \n",
    "Finally, you will describe, evaluate, and interpret the results obtained from the two RL agents. Additionally, compare your agents with the provided Random Agent. Feel free to utilize the provided helper functions for evaluating your agents. Some important points are:\n",
    "\n",
    "- Both quantified evaluation and human evaluation are needed in the report. The quantified evaluation should focus on the measurement of reward. In the human evaluation, you can use the provided visual tools to interpret your results. Your report should include at least one plot presenting comparable measures for the different agents.\n",
    "\n",
    "- While evaluating the results of Agent 2 (with SARSA algorithm), please try at least 2 different values of **epsilon** (expect 0) and discuss the influence of different epsilon values on the results. In the end, please identify a reasonable epsilon value that could balance the exploration and exploitation, then fix this value for comparing the two agents. Present your trails and results in the report.\n",
    "\n",
    "- In the report, you also need to parcitularly describe and discuss the similarity and difference of results from two RL agents (hint: on-policy VS off-policy). For this, please make sure that the compared results are obtained from the same environment (the same maze for two different agents). Also, while evaluating the results of two agents, please try at least 2 different values of **gamma**. In this way, you could discuss the influence of this discount factor in your report. \n",
    "\n",
    "- Please run the simulation multiple times and average the results for all your findings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### FUNCTIONS FOR PARAMETER TUNING ################################################\n",
    "\n",
    "# Trains X agents in the same maze and visualizes the average state values\n",
    "def visualize_average_state_values(agent, X=5, algorithm=\"Empty\"):\n",
    "    total_state_values = np.zeros_like(agent.q_table.max(axis=2))\n",
    "    \n",
    "    for _ in range(X):\n",
    "        agent.reset() # Reset q-table\n",
    "        agent.train()  # Train the agent again\n",
    "        total_state_values += agent.q_table.max(axis=2)  # Add state values\n",
    "\n",
    "    avg_state_values = total_state_values / X\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.set_title(f\"Average Highest State Value at position (x,y)\")\n",
    "    sns.heatmap(avg_state_values, ax=ax)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "\n",
    "def train_multiple(agent, exploration_rate=0.1, discount_rate=0.95, n_episodes=10_000, N=5):\n",
    "    agent.epsilon = exploration_rate\n",
    "    agent.gamma = discount_rate\n",
    "    for i in range(N):\n",
    "        agent.reset()\n",
    "        total_rewards_per_episode = agent.train(n_episodes = n_episodes)\n",
    "    avg_total_rewards_per_episode = [elem / N for elem in total_rewards_per_episode]\n",
    "    return avg_total_rewards_per_episode\n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "def plot_exploration_test_comparison(epsilons, rewards_list, algorithm=\"Empty\"):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for i, rewards in enumerate(rewards_list):\n",
    "        smoothed_rewards = np.convolve(rewards, np.ones(100), mode='valid')\n",
    "        plt.plot(smoothed_rewards, label=f\"Epsilon = {epsilons[i]}\")\n",
    "    \n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Mean Total Reward\")\n",
    "    plt.title(f\"Effect of Exploration Rate on Total Reward with {algorithm}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_discount_test_comparison(discounts, rewards_list, algorithm=\"Empty\"):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for i, rewards in enumerate(rewards_list):\n",
    "        smoothed_rewards = np.convolve(rewards, np.ones(100), mode='valid')\n",
    "        plt.plot(smoothed_rewards, label=f\"Discount = {discounts[i]}\")\n",
    "    \n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Mean Total Reward\")\n",
    "    plt.title(f\"Effect of Discount Rate on Total Reward with {algorithm}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def optimize(agent):\n",
    "    agent.epsilon = 0.1\n",
    "    agent.gamma = 0.95\n",
    "    return agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of exploration rate on SARSA and Q-learning\n",
    "epsilons = [0.1, 0.3, 0.5]\n",
    "exploration_episodes = 15_000\n",
    "rewards_sarsa_0_1 = train_multiple(sarsa_agent, exploration_rate=epsilons[0], n_episodes=exploration_episodes)\n",
    "rewards_sarsa_0_3 = train_multiple(sarsa_agent, exploration_rate=epsilons[1], n_episodes=exploration_episodes)\n",
    "rewards_sarsa_0_5 = train_multiple(sarsa_agent, exploration_rate=epsilons[2], n_episodes=exploration_episodes)\n",
    "\n",
    "rewards_qlearning_0_1 = train_multiple(q_agent, exploration_rate=epsilons[0], n_episodes=exploration_episodes)\n",
    "rewards_qlearning_0_3 = train_multiple(q_agent, exploration_rate=epsilons[1], n_episodes=exploration_episodes)\n",
    "rewards_qlearning_0_5 = train_multiple(q_agent, exploration_rate=epsilons[2], n_episodes=exploration_episodes)\n",
    "\n",
    "plot_exploration_test_comparison(epsilons, [rewards_sarsa_0_1, rewards_sarsa_0_3, rewards_sarsa_0_5], algorithm=\"SARSA\")\n",
    "plot_exploration_test_comparison(epsilons, [rewards_qlearning_0_1, rewards_qlearning_0_3, rewards_qlearning_0_5], algorithm=\"Q-Learning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of discount rate on SARSA and q-learning\n",
    "discounts = [0.90, 0.95, 0.99]\n",
    "discount_episodes = 10_000\n",
    "rewards_sarsa_0_90 = train_multiple(sarsa_agent, discount_rate=discounts[0], n_episodes=discount_episodes)\n",
    "rewards_sarsa_0_95 = train_multiple(sarsa_agent, discount_rate=discounts[1], n_episodes=discount_episodes)\n",
    "rewards_sarsa_0_99 = train_multiple(sarsa_agent, discount_rate=discounts[2], n_episodes=discount_episodes)\n",
    "\n",
    "rewards_qlearning_0_90 = train_multiple(q_agent, discount_rate=discounts[0], n_episodes=discount_episodes)\n",
    "rewards_qlearning_0_95 = train_multiple(q_agent, discount_rate=discounts[1], n_episodes=discount_episodes)\n",
    "rewards_qlearning_0_99 = train_multiple(q_agent, discount_rate=discounts[2], n_episodes=discount_episodes)\n",
    "\n",
    "plot_discount_test_comparison(discounts, [rewards_sarsa_0_90, rewards_sarsa_0_95, rewards_sarsa_0_99], algorithm=\"SARSA\")\n",
    "plot_discount_test_comparison(discounts, [rewards_qlearning_0_90, rewards_qlearning_0_95, rewards_qlearning_0_99], algorithm=\"Q Learning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show q-table for optimal hyperparameter\n",
    "q_agent_optimal = optimize(q_agent)\n",
    "visualize_average_state_values(q_agent_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show q-table for optimal hyperparameters\n",
    "sarsa_agent_optimal = optimize(sarsa_agent)\n",
    "visualize_average_state_values(sarsa_agent_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Agent\n",
    "random_agent = RandomAgent(train_env)\n",
    "rewards_randomagent_0_1 = train_multiple(random_agent, exploration_rate=epsilons[0])\n",
    "rewards_randomagent_0_3 = train_multiple(random_agent, exploration_rate=epsilons[1])\n",
    "rewards_randomagent_0_5 = train_multiple(random_agent, exploration_rate=epsilons[2])\n",
    "rewards_randomagent_0_90 = train_multiple(random_agent, discount_rate=discounts[0])\n",
    "rewards_randomagent_0_95 = train_multiple(random_agent, discount_rate=discounts[1])\n",
    "rewards_randomagent_0_99 = train_multiple(random_agent, discount_rate=discounts[2])\n",
    "\n",
    "\n",
    "plot_exploration_test_comparison(epsilons, [rewards_randomagent_0_1, rewards_randomagent_0_3, rewards_randomagent_0_5], algorithm=\"Random\")\n",
    "plot_discount_test_comparison(discounts, [rewards_randomagent_0_90, rewards_randomagent_0_95, rewards_randomagent_0_99], algorithm=\"Random\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Tasks \n",
    "\n",
    "We would like to challenge you with the following bonus task. For each task that is successfully completed, you may obtain max. 1 extra point. \n",
    "\n",
    "1. Implement a third RL agent using another RL algorithm (e.g. Monte Carlo methods, Expected SARSA or even neural network-based ones) and discuss your findings. Compare this third agent with the above ones and explain why this is a better (or worse) RL algorithm. You are allowed to reuse exsiting packages, but please cite them, test them in advance, and make sure that you can explain the used algorithm using your own words.\n",
    "\n",
    "2. Can you explore and show other evaluation results? If so, implement and present one extra result (e.g. a plot). And please explain why it is a good evaluation method for our task or how it shows the difference between two RL agents/algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
